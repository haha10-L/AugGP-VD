{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "from time import time\n",
    "import math\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score, roc_auc_score\n",
    "from Utils.pytorchtools import EarlyStopping  #location: /utils/\n",
    "from tqdm import tqdm  #用于在循环或迭代过程中显示进度条\n",
    "from transformers import BertTokenizer, BertModel\n",
    "#from torch_scatter import scatter_max, scatter_add\n",
    "import networkx as nx\n",
    "#from torch_geometric.nn import GCNConv   后面有用到，由于服务器出错这里\n",
    "#import dgl\n",
    "#import cugraph\n",
    "#import cudf"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"无序列化\"\"\"\n",
    "class Graph():\n",
    "    def __init__(self,num_nodes,edge_pairs,features,label,node_degs):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.edge_pairs = edge_pairs\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self.node_degs = node_degs  #邻接点数量\n",
    "        \n",
    "    def __str__(self):\n",
    "        return ('nodes: %d  edge_pairs: %d features: %d' % (self.num_nodes,len(self.edge_pairs),len(self.features)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"序列化\"\"\"\n",
    "class Graph_sort():\n",
    "    def __init__(self,num_nodes,features):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.features = features\n",
    "        \n",
    "    def __str__(self):\n",
    "        return ('nodes: %d  features: %d' % (self.num_nodes,len(self.features)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#读取图数据并构建一个Graph对象\n",
    "def getGraph(filename):\n",
    "    f = open(filename)\n",
    "    row = f.readline().strip().split()\n",
    "    nodes,label = [int(w) for w in row]\n",
    "    node_features = []\n",
    "    edge_pairs = []\n",
    "    node_degs = []\n",
    "    for j in range(nodes):\n",
    "        row = f.readline().strip().split()\n",
    "        node_deg = int(row[0])+1\n",
    "        row ,attr = [int(w) for w in row[1:int(row[0])+1]],np.array([float(w) for w in row[int(row[0])+1:]])\n",
    "        if attr is not None:\n",
    "            node_features.append(attr)\n",
    "        if node_deg is not None:\n",
    "            node_degs.append(node_deg)\n",
    "        if row is not None:\n",
    "            for k in row:\n",
    "                edge = [j,k]\n",
    "                edge_pairs.append(edge)\n",
    "    g = Graph(nodes,edge_pairs,node_features,label,node_degs)\n",
    "    return g"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"无序列化\"\"\"\n",
    "def getGraphList(files):\n",
    "    glist = []\n",
    "    for file in files:\n",
    "        graph = getGraph(file)\n",
    "        glist.append(graph)\n",
    "    return glist"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def merge_Graph(graph_list, files_n, edge_folder):\n",
    "    prefix_sum = []  # 每个图数据中节点数量的和\n",
    "    node_features = []  # 节点特征\n",
    "    node_degs = []  # 节点度数\n",
    "    node_labels = []  # 节点标签\n",
    "    total_num_edges = 0  # 总边数\n",
    "    total_num_nodes = 0  # 总节点数\n",
    "    edge_pairs = []  # 每个图数据的边信息的列表\n",
    "    graph_sizes = []  # 存储每个图数据的节点数量\n",
    "    edge_features_list = []  # 存储边特征列表\n",
    "\n",
    "    for i in range(len(graph_list)):\n",
    "        prefix_sum.append(graph_list[i].num_nodes)\n",
    "        if i != 0:\n",
    "            prefix_sum[i] += prefix_sum[i-1]\n",
    "        node_features.extend(graph_list[i].features)\n",
    "        node_degs.extend(graph_list[i].node_degs)\n",
    "        node_labels.append(graph_list[i].label)\n",
    "        total_num_edges += len(graph_list[i].edge_pairs)\n",
    "        total_num_nodes += graph_list[i].num_nodes\n",
    "        graph_sizes.append(graph_list[i].num_nodes)\n",
    "        edge_pairs.append(graph_list[i].edge_pairs)\n",
    "\n",
    "        # Load edge features\n",
    "        sanitized_filename = files_n[i].split('/')[-1].replace('.txt', '')  \n",
    "        edge_file_path = os.path.join(edge_folder, sanitized_filename + '.txt')\n",
    "        with open(edge_file_path, 'r') as edge_file:\n",
    "            lines = edge_file.readlines()\n",
    "            edge_features = {int(line.split()[0]): torch.zeros(len(line.split()) - 1) for line in lines[1:]}\n",
    "            for line in lines[1:]:\n",
    "                i, *edge_feature = map(float, line.split())\n",
    "                edge_features_list.append((int(i), torch.tensor(edge_feature)))\n",
    "            \n",
    "\n",
    "    # create batch_graph\n",
    "    n2n_idxes = torch.LongTensor(2, total_num_edges)  \n",
    "    n2n_vals = torch.FloatTensor(total_num_edges)  \n",
    "    \n",
    "    for i in range(len(graph_list)):\n",
    "        prefix_sum[len(graph_list)-i-1] = prefix_sum[len(graph_list)-i-2]\n",
    "    prefix_sum[0] = 0\n",
    "    \n",
    "    for i in range(total_num_edges):\n",
    "        n2n_vals[i] = 1\n",
    "    \n",
    "    j = 0\n",
    "    \n",
    "    for i in range(len(graph_list)):\n",
    "        for k, item in enumerate(edge_pairs[i]):\n",
    "            n2n_idxes[0][j] = item[0] + prefix_sum[i]\n",
    "            n2n_idxes[1][j] = item[1] + prefix_sum[i]\n",
    "            j += 1\n",
    "            \n",
    "    merged_edge_features = []\n",
    "\n",
    "    for _, edge_feature in edge_features_list:\n",
    "        merged_edge_features.append(edge_feature)\n",
    "\n",
    "    n2n = torch.sparse.FloatTensor(n2n_idxes, n2n_vals, torch.Size([total_num_nodes, total_num_nodes]))\n",
    "    node_features = torch.FloatTensor(node_features)\n",
    "    merged_edge_features = torch.stack(merged_edge_features)\n",
    "    \n",
    "    indices = n2n._indices()  \n",
    "    values = n2n._values() \n",
    "    in_degrees = torch.zeros(total_num_nodes, device=n2n.device)\n",
    "    in_degrees.index_add_(0, indices[1], values)  \n",
    "    in_degrees_inv = torch.pow(in_degrees, -1)\n",
    "    in_degrees_inv[torch.isinf(in_degrees_inv)] = 0  \n",
    "    \n",
    "    \n",
    "\n",
    "    # 创建稀疏度矩阵的索引\n",
    "    degs_index = torch.arange(total_num_nodes, device=n2n.device)\n",
    "    degs_index = torch.stack([degs_index, degs_index], dim=0)\n",
    "    \n",
    "    # 构建稀疏度矩阵，按入度计算的\n",
    "    node_degs = torch.sparse.FloatTensor(degs_index, in_degrees_inv, torch.Size([total_num_nodes, total_num_nodes]))  \n",
    "        \n",
    "    return n2n, node_features, node_degs, graph_sizes, merged_edge_features,in_degrees_inv  # merged_edge_features\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"序列化图的合并\"\"\"\n",
    "def merge_Graph_sort(graph_list):\n",
    "    node_features = []  # 节点特征\n",
    "    graph_sizes = []  # 每个图的节点数\n",
    "    \n",
    "    for graph in graph_list:\n",
    "        node_features.extend(graph.features)\n",
    "        graph_sizes.append(graph.num_nodes)\n",
    "    \n",
    "    # 将节点特征向量转换为张量\n",
    "    node_features = torch.FloatTensor(node_features)\n",
    "    \n",
    "    return node_features, graph_sizes\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features,alpha=0.5, num_iterations=2, activation=None, bias=True):#edge_feat_dim\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.num_iterations = num_iterations\n",
    "        self.activation = activation\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.edge_weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        \n",
    "        self.biLinear = nn.Bilinear(out_features, out_features, out_features)\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.edge_weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, features, adj, degs, edge_features ,graph_sizes):#edge_features\n",
    "        # Add self-loops to the adjacency matrix (A + I)\n",
    "        identity = torch.eye(adj.size(0)).to_sparse().to(adj.device)\n",
    "        adj = adj + identity\n",
    "\n",
    "        # Transpose adjacency matrix for in-degree normalization\n",
    "        adj = adj.t()\n",
    "        \n",
    "        # Initialize normalized adjacency matrix\n",
    "        norm_indices = adj._indices()\n",
    "        norm_values = adj._values().clone()\n",
    "\n",
    "        # Calculate normalized adjacency matrix\n",
    "        start = 0\n",
    "        for size in graph_sizes:\n",
    "            end = start + size\n",
    "            sub_adj_indices = norm_indices[:, (norm_indices[0] >= start) & (norm_indices[0] < end)]\n",
    "            sub_adj_indices = sub_adj_indices - start\n",
    "            sub_adj_values = norm_values[(norm_indices[0] >= start) & (norm_indices[0] < end)]\n",
    "            sub_adj = torch.sparse.FloatTensor(sub_adj_indices, sub_adj_values, torch.Size([size, size]))\n",
    "            row, col = sub_adj._indices()\n",
    "            deg = torch.sparse.sum(sub_adj, dim=1).to_dense()\n",
    "            deg_inv_sqrt = torch.pow(deg, -0.5)\n",
    "            deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "            norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "            norm_values[(norm_indices[0] >= start) & (norm_indices[0] < end)] = norm\n",
    "            start = end\n",
    "\n",
    "        adj_norm = torch.sparse.FloatTensor(norm_indices, norm_values, adj.size())\n",
    "\n",
    "        # Node feature transformation\n",
    "        support = torch.mm(features, self.weight)\n",
    "        \n",
    "        # Initial feature matrix\n",
    "        H0 = support.clone()\n",
    "        \n",
    "        H = support\n",
    "        # Aggregate neighbor node features using sparse matrix multiplication\n",
    "        for _ in range(self.num_iterations):\n",
    "            aggregated_neighbors = torch.mm(adj_norm, H)\n",
    "            H = self.alpha * aggregated_neighbors + (1 - self.alpha) * H0\n",
    "\n",
    "        # Transform edge features\n",
    "        transformed_edge_features = torch.mm(edge_features, self.edge_weight)\n",
    "        \n",
    "        # Ensure edge features have the correct dimensions\n",
    "        if transformed_edge_features.size(0) != features.size(0):\n",
    "            raise ValueError(\"Edge features size must match the number of nodes\")\n",
    "\n",
    "        # Combine node features and edge aggregated features\n",
    "        #updated_features = H + transformed_edge_features\n",
    "        #updated_features = H\n",
    "        updated_features = self.biLinear(H, transformed_edge_features)\n",
    "\n",
    "        # Apply degree matrix\n",
    "        output = torch.mm(degs, updated_features)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        \n",
    "        return output, transformed_edge_features\n",
    "        #return output\n",
    "        \n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "class SelfAttentionPool(nn.Module):\n",
    "    def __init__(self, feature_dim, pooled_dim):\n",
    "        super(SelfAttentionPool, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.pooled_dim = pooled_dim\n",
    "        \n",
    "        # 自注意力机制所需的查询、键、值线性层\n",
    "        self.query_layer = nn.Linear(feature_dim, pooled_dim)\n",
    "        self.key_layer = nn.Linear(feature_dim, pooled_dim)\n",
    "        self.value_layer = nn.Linear(feature_dim, pooled_dim)\n",
    "        \n",
    "        # 用于聚合被丢弃节点的特征\n",
    "        self.linear_agg = nn.Linear(feature_dim, pooled_dim)\n",
    "        self.norm_layer = nn.LayerNorm(pooled_dim)  # 归一层\n",
    "        \n",
    "        # 用于计算丢弃节点的权重\n",
    "        self.weight_layer = nn.Linear(feature_dim, 1)\n",
    "\n",
    "    def forward(self, x, graph_sizes, in_degrees):\n",
    "        start_idx = 0\n",
    "        output_features = []\n",
    "        \n",
    "        for size in graph_sizes:\n",
    "            end_idx = start_idx + size\n",
    "            x_graph = x[start_idx:end_idx]\n",
    "            in_deg_graph = in_degrees[start_idx:end_idx]\n",
    "\n",
    "            # 计算查询、键和值\n",
    "            queries = self.query_layer(x_graph)\n",
    "            keys = self.key_layer(x_graph)\n",
    "            values = self.value_layer(x_graph)\n",
    "\n",
    "            # 计算注意力分数\n",
    "            attn_scores = torch.matmul(queries, keys.transpose(0, 1)) / (self.pooled_dim ** 0.5)  # 缩放点积注意力\n",
    "            attn_scores = attn_scores.mean(dim=1)  # 每个节点的注意力分数\n",
    "            \n",
    "            # 结合入度信息\n",
    "            attn_scores = attn_scores * in_deg_graph  # 将注意力分数与入度相乘\n",
    "            attn_scores = F.softmax(attn_scores, dim=0)\n",
    "\n",
    "            # 对节点特征进行加权求和\n",
    "            x_pooled = torch.sum(attn_scores.unsqueeze(1) * values, dim=0, keepdim=True)\n",
    "\n",
    "            # 归一化与激活\n",
    "            x_pooled = self.norm_layer(x_pooled)\n",
    "            x_pooled = F.relu(x_pooled)\n",
    "            #x_pooled = F.leaky_relu(x_pooled)\n",
    "\n",
    "            output_features.append(x_pooled)\n",
    "\n",
    "            start_idx = end_idx\n",
    "\n",
    "        output_features = torch.cat(output_features, dim=0)\n",
    "        \n",
    "        return output_features"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class AugGcn(nn.Module):\n",
    "    def __init__(self, feature_dim, hid_dim, out_dim):\n",
    "        super(AugGcn, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(feature_dim)\n",
    "                \n",
    "        self.conv1 = GraphConvolution(feature_dim, hid_dim,activation = F.leaky_relu)\n",
    "        self.bn1 = nn.BatchNorm1d(hid_dim)\n",
    "        \n",
    "        self.conv2 = GraphConvolution(hid_dim, hid_dim,activation = F.leaky_relu)\n",
    "        self.bn2 = nn.BatchNorm1d(hid_dim)\n",
    "        \n",
    "        self.conv3 = GraphConvolution(hid_dim, out_dim,activation = F.leaky_relu)\n",
    "\n",
    "    def forward(self, features, graph_adj, deg, edge_f,graph_sizes, in_degree):\n",
    "        \n",
    "        features = self.bn(features)\n",
    "        \n",
    "        feat1,edge_f1 = self.conv1(features, graph_adj, deg, edge_f, graph_sizes) \n",
    "        feat1 = self.bn1(feat1)\n",
    "        \n",
    "        feat2,edge_f2 = self.conv2(feat1, graph_adj, deg, edge_f1, graph_sizes) \n",
    "        feat2 = self.bn2(feat2)\n",
    "        \n",
    "        feat3,edge_f3= self.conv3(feat2, graph_adj, deg, edge_f2, graph_sizes)\n",
    "        \n",
    "        return feat3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"分类模型\"\"\"\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self,classNum,dropout_rate,nfeat,nhid,out_dim):\n",
    "        super(Classifier,self).__init__()\n",
    "        self.classNum = classNum\n",
    "        self.dropout_rate = dropout_rate\n",
    "      \n",
    "        self.g_s = AugGcn(nfeat, nhid, out_dim)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(128)\n",
    "        self.denseLayer1 = nn.Linear(128,256) \n",
    "        self.dropout1 = nn.Dropout(p = self.dropout_rate)\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.denseLayer2 = nn.Linear(256,64)\n",
    "        self.dropout2 = nn.Dropout(p = self.dropout_rate)\n",
    "        \n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.outputLayer = nn.Linear(64,classNum) \n",
    "\n",
    "    def forward(self,features, graphs,degs,graph_sizes, edge_f,in_degree):#edge_f\n",
    "    \n",
    "        feats = self.g_s(features, graphs, degs, edge_f,graph_sizes, in_degree)#edge_f\n",
    "     \n",
    "        features = self.sa_pool(feats, graph_sizes, in_degree)\n",
    "       \n",
    "        res = features\n",
    "       \n",
    "        res = self.bn(res)\n",
    "        \n",
    "        res = F.relu(self.denseLayer1(res))\n",
    "        res = self.dropout1(res)\n",
    "        \n",
    "        res = self.bn2(res)\n",
    "        res = F.relu(self.denseLayer2(res))\n",
    "        res = self.dropout2(res)\n",
    "        \n",
    "        res = self.bn4(res)\n",
    "        output = self.outputLayer(res)\n",
    "        \n",
    "        return output.flatten() \n",
    "     "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "path = '../Data/Graph_normal/'        # 无漏洞\n",
    "pathA = '../Data/Graph_vulnerable/'    # 有漏洞\n",
    "\n",
    "files = os.listdir(path)\n",
    "filesA = os.listdir(pathA)\n",
    "\n",
    "graph_filename = [path+file for file in files]\n",
    "graph_filename.extend([pathA+file for file in filesA])\n",
    "\n",
    "all_label = [0.0 for i in range(len(files))]\n",
    "all_label.extend([1.0 for i in range(len(filesA))]) \n",
    "length = len(files) + len(filesA)\n",
    "\n",
    "from sklearn.utils import shuffle  \n",
    "graph_filename,all_label = shuffle(graph_filename,all_label)\n",
    "\n",
    "k = 0.8\n",
    "k1 = 0.9\n",
    "\n",
    "train_graph_data = graph_filename[0:int(k*length)]  \n",
    "train_label = all_label[0:int(k*length)]\n",
    "\n",
    "test_graph_data = graph_filename[int(k*length):] \n",
    "test_label = all_label[int(k*length):]\n",
    "\n",
    "len(train_graph_data),len(test_graph_data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def evaluate(predict,t_label,thresh):  \n",
    "\n",
    "    pre = (predict >= thresh).float()\n",
    "    accuracy = accuracy_score(t_label.data.cpu(),pre.data.cpu())\n",
    "    precision = precision_score(t_label.data.cpu(),pre.data.cpu(), zero_division = 0)\n",
    "    recall = recall_score(t_label.data.cpu(),pre.data.cpu(), zero_division = 0)\n",
    "    f1 = f1_score(t_label.data.cpu(),pre.data.cpu(), zero_division = 0)\n",
    "    return precision, recall, f1, accuracy #without auc\n",
    "\n",
    "def test_evaluate(predict,t_label,thresh):  \n",
    "    pre = (predict >= thresh).float()\n",
    "#     print(predict,t_label)\n",
    "    accuracy = accuracy_score(t_label.data.cpu(),pre.data.cpu())\n",
    "    precision = precision_score(t_label.data.cpu(),pre.data.cpu(), zero_division = 0)\n",
    "    recall = recall_score(t_label.data.cpu(),pre.data.cpu(), zero_division = 0)\n",
    "    f1 = f1_score(t_label.data.cpu(),pre.data.cpu(), zero_division = 0)\n",
    "    # fpr,tpr,_ = roc_curve(t_label.data.cpu(),predict.data.cpu())\n",
    "    auc = roc_auc_score(t_label.data.cpu(),predict.data.cpu())\n",
    "\n",
    "    return precision, recall, f1, accuracy, auc "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def test():\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        graphs,features,node_degs,graph_sizes, edge_f,in_degree = merge_Graph(getGraphList(test_graph_data), test_graph_data, '../Data/Graph_edge/')\n",
    "        graphs = graphs.cuda()\n",
    "        labels = torch.FloatTensor(test_label).cuda()\n",
    "        node_degs = node_degs.cuda()\n",
    "        features = features.cuda()\n",
    "        edge_f = edge_f.cuda()\n",
    "        in_degree = in_degree.cuda()\n",
    "        \n",
    "            \n",
    "        graphs = Variable(graphs)\n",
    "        node_degs = Variable(node_degs)\n",
    "        features = Variable(features)\n",
    "        edge_f = Variable(edge_f)\n",
    "        in_degree = Variable(in_degree)\n",
    "\n",
    "         \n",
    "        output = model(features, graphs, node_degs, graph_sizes,edge_f,in_degree)  #调用forward\n",
    "        l = loss(output,labels)\n",
    "        precesion,recall,f1_score,acc,auc = test_evaluate(torch.sigmoid(output),labels,threshold)\n",
    "\n",
    "    return precesion,recall,f1_score,acc,auc\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    start = time()\n",
    "    accuracy = []\n",
    "    j = 0\n",
    "    best_loss=1\n",
    "    for graph_files ,labels in zip([train_graph_data[i:i + batch_size] for i in range(0, len(train_graph_data),batch_size)],[train_label[i:i + batch_size] for i in range(0, len(train_label), batch_size)]):\n",
    "\n",
    "        j += 1\n",
    "        ss = time()\n",
    "        \n",
    "        graphs,features,node_degs,graph_sizes, edge_f,in_degree = merge_Graph(getGraphList(graph_files),graph_files,'../Data/Graph_edge/') #edge_f,\n",
    "        \n",
    "        graphs = graphs.cuda()      \n",
    "        node_degs = node_degs.cuda()\n",
    "        features = features.cuda()\n",
    "        edge_f = edge_f.cuda()\n",
    "        in_degree = in_degree.cuda()\n",
    "        \n",
    "        labels = torch.FloatTensor(labels).cuda()\n",
    "        \n",
    "        graphs = Variable(graphs)\n",
    "        node_degs = Variable(node_degs)\n",
    "        features = Variable(features)\n",
    "        edge_f = Variable(edge_f)\n",
    "        in_degree = Variable(in_degree)\n",
    "        \n",
    "        output = model(features, graphs, node_degs, graph_sizes, edge_f,in_degree)  #调用forward\n",
    "        \n",
    "        l = loss(output,labels)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        \n",
    "        if(best_loss<=l.item()):\n",
    "            model.save_to_state_dict(torch.load('checkpoint.pt'))\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        pre,rec,f1, acc = evaluate(output,labels,threshold)\n",
    "        accuracy.append(acc)\n",
    "        \n",
    "        ee = time()\n",
    "\n",
    "    accuracy = np.array(accuracy).mean()\n",
    "    end = time()\n",
    "    print('epoch %d, train_loss : %f trainacc: %f ' % (epoch+1, l.item(), accuracy))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = Classifier(\n",
    "    classNum = 1,  #类别数\n",
    "    dropout_rate=0.5, \n",
    "    nfeat = 100, #输入特征维度\n",
    "    nhid = 64, #隐藏大小\n",
    "    out_dim = 128, #输出维度\n",
    ")\n",
    "sortk = 200\n",
    "batch_size = 32 #32  \n",
    "threshold = 0.6  #阈值，用于二分类，需要实现寻找最佳的阈值 0.45，尝试设置的高一点0.9\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "loss = nn.BCEWithLogitsLoss()    # This loss combines a Sigmoid layer and the BCELoss in one single class. 二进制交叉熵损失函数\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.1)  \n",
    "scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.0004 ,cycle_momentum = False)   \n",
    "\n",
    "model.parameters"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "train_loss = []\n",
    "test_acc = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "l_rs = []\n",
    "Precesion = []\n",
    "Recall = []\n",
    "F1_score = []\n",
    "AUC = []\n",
    "#wf = './BFS_EA_RGCN(SG)/'\n",
    "wf='../Data/Record/'\n",
    "early_stop_flag = False"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def start_training():\n",
    "    global train_graph_data,train_label,early_stop_flag,model\n",
    "    for j in range(100):\n",
    "        train_graph_data,train_label = shuffle(train_graph_data,train_label)\n",
    "        train(j)\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#训练了30次\n",
    "for i in tqdm(range(30)):\n",
    "    s_time = time()\n",
    "    graph_filename, all_label = shuffle(graph_filename,all_label)\n",
    "\n",
    "    train_graph_data = graph_filename[0:int(k*length)]  #训练集80%\n",
    "    train_label = all_label[0:int(k*length)]    \n",
    "\n",
    "    test_graph_data = graph_filename[int(k*length):]  #测试集10% 20%\n",
    "    test_label = all_label[int(k*length):]\n",
    "    \n",
    "    model = Classifier(\n",
    "    classNum = 1,\n",
    "    dropout_rate=0.5,\n",
    "    nfeat = 100, \n",
    "    nhid = 64, \n",
    "    out_dim = 128,\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    loss = nn.BCEWithLogitsLoss()    \n",
    "    optimizer = optim.Adam(model.parameters(),lr = 0.1)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.0004 ,cycle_momentum = False)\n",
    "    \n",
    "    start_training()\n",
    "    \n",
    "    pre,recall,f1,acc,auc = test() \n",
    "    e_time = time()\n",
    "    f = open(wf+'Mytest_result-2.txt','a')\n",
    "    print('exp-%d acc: %f  precesion: %f  recall: %f  f1_score: %f auc: %f\\n' % (i,acc, pre, recall, f1,auc),file = f)\n",
    "    print('exp-%d acc: %f  precesion: %f  recall: %f  f1_score: %f auc: %f time: %f' % (i,acc, pre, recall, f1,auc, e_time - s_time))\n",
    "    test_acc.append(acc)\n",
    "    Precesion.append(pre)\n",
    "    Recall.append(recall)\n",
    "    F1_score.append(f1)\n",
    "    AUC.append(auc)\n",
    "    f.close()\n",
    "    \n",
    "f = open(wf+'Mytest_result.txt','a')\n",
    "print('ave_acc: %f  ave_precesion: %f  ave_recall: %f  ave_f1_score: %f ave_auc: %f\\n' % (np.array(test_acc).mean(), np.array(Precesion).mean(), np.array(Recall).mean(), np.array(F1_score).mean(),np.array(AUC).mean()),file=f)\n",
    "print('var_acc: %f  var_precesion: %f  var_recall: %f  var_f1_score: %f var_auc: %f\\n' % (np.array(test_acc).var(), np.array(Precesion).var(), np.array(Recall).var(), np.array(F1_score).var(),np.array(AUC).var()),file=f)\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "torch.save(model, wf+'GcnP.pkl')",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
